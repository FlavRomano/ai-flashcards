---
tags: [card, evolutionary, aif]
cards-deck: aif::evo1
---
# Evolutionary I
## What is artificial life? #card  
- Is a branch of AI that attempts to understand **living systems properties**
- Translates those properties into software (**Software Artificial Life**)

## What's the Biological Algorithm? #card 
Darwin theory told us that a species can evolve under **3 necessary and sufficient conditions**.
1. Differential reproduction
2. Heredity
3. Variation

### 1. Differential reproduction #card 
Only a subset of the population can reproduce, not necessarily the fittest of them.

### 2. Heredity #card 
Parents features go to offsprings.

### 3. Variation #card 
Different people have different features
- reproduction should create mixed features
- if we are all equals, there is no evolution

## What's mutation? #card 
- Mutation is randomness added to genes
- Offsprings will have slightly different traits then parents
	- thanks to mutation, obv.

## What's recombination? #card 
- Recombination is 
	- **mixing of genes** that **creates new genes**
- it's genetic shuffling between two individuals

## Why mutation+recombination is a divergent process? #card 
- Each time we have new traits to transfer to the offspring
- It does not collapse to a mean

## Evolutionary algorithm #card 
EA can solve optimization problem through evolution of solutions, its population.

### How EA works? #card 
1. Select a population $P_0$
2. Evaluate their fitness (how well the solution solves the problem)
3. Loop from t in 0...:
	1. Select a parents $p\in P_t$
	2. make offsprings $x_i$ through **Reproduction + Recombination**
	3. apply **mutation** to feature
	4. evaluate fitness of new population
	5. $P_{t+1} = \text{survived at 4.}$, 

### What's fitness function? #card 
- Is an objective function for a problem
- Is used to evaluate
	- how **close a solution** is achieving a **set of aims**
- Guide the evolutionary development towards a goal

### What subclasses of Evolutionary Algorithms exists? #card 
3 subclasses:
1. Evolutionary Strategy
2. Genetic Algorithms
3. Genetic Programming

### Evolutionary Strategy #card 
- Are based on a population distribution
- Are inspired by natural selection
	- loop random mutation + evaluation + recombination
- Are used for continuous optimization problems
	- self-adapting mutation rate (sigmas)
	- along with the solution
- Treat mutation as noise $$x_{new} = x_{old} + N(m, \sigma^2 I)$$

#### What's black box optimization? #card 
- Is a branch of optimization
- Doesn't use derivative to find optimal solutions
	- the only information is the evaluation of $f$ in some points
- Evolutionary strategy 

#### What's $\sigma$? #card 
- Is the **mutation rate** $$huge\; \sigma \implies \textbf{huge steps}$$ $$small\; \sigma \implies \textbf{small steps}$$
- is passed to the offsprings if parent survive.
- affects the speed of mutation $$x_{new} = x_{old} + N(m, \sigma^2 I)$$ the gaussian noise is spherical, centered on $m$

#### How parents are selected? #card 
There are different technique of selection:
- comma selection $(\mu, \lambda)$
- plus selection $(\mu+\lambda)$

##### Comma selection $(\mu,\lambda)$ #card
For all generation $g$: 
1. Discard parents, keep offsprings $$\mu=\#parents$$ $$\lambda=\#children$$
2. Select the best $\mu$  children
3. For all k in children
	1. adapting mutation rate $$\sigma_k^{g+1} = \text{recombine}(\sigma^g \mid P^g) \cdot N(0,1)$$
	2. recombination + mutation of offsprings $$x_k^{g+1} = \text{recombine}(P^g) + N(0,\sigma_k^{g+1})$$

###### Why Comma selection is good? #card 
- Is non-elitarian, prevents stagnation
	- parents can't be selected over kids
	- even if they are better
- explores more, exploits less
	- we prevent local optima stuck

##### Plus selection $(\mu + \lambda)$ #card
For all generation $g$: 
1. keep parents, keep offsprings $$\mu=\#parents$$ $$\lambda=\#children$$
2. Select the best $\mu$  individuals from parents + offspring pool
3. For all k in children
	1. adapting mutation rate $$\sigma_k^{g+1} = \text{recombine}(\sigma^g \mid P^g) \cdot N(0,1)$$
	2. recombination + mutation of offsprings $$x_k^{g+1} = \text{recombine}(P^g) + N(0,\sigma_k^{g+1})$$

###### What is a cons of plus selection $(\mu + \lambda)$? #card 
- Preserve the best, elitist
	- if parents are better then offspring, exploits them
	- risks of getting stuck and converge prematurely

#### What kind of noise standard ES adds? #card 
- Gaussian noise $$N(0, \sigma)$$ a sphere centered in zero and $\sigma$ wide
- The search is spherical, same intensity on all directions
	- for sparse problems this is very inefficient.
	- a solution is CMA ES

#### What's CMA ES? #card 
- Covariance Matrix Adaptation Evolutionary Strategy
	- based on the adaptation of the covariance matrix through generations
- Instead of spherical noise (inefficient search for sparse problem) 
	- each time we adds a noise shape
		- that **fits** the best based on previous knowledge $$N(m^g, C^g)$$
		- **momentum like**

##### What is covariance matrix? #card 
- Is a symmetric diagonal matrix which has
	- diagonal elements=variance of each variable $$x_{ij} = \sigma_{x_i}^2\quad i=j$$ by definition, positive or null.
		- big variance => big steps, small variance => small step
	- otherwise=covariance of each variables couple $$x_{ij} = Cov(X,Y)$$ could be positive, negative or zero.
		- the upper diagonal is symmetrical to the lower diagonal, because $$Cov(X,Y)=Cov(Y,X)$$
		- measure 
- In Evolutionary Strategy defines shape and direction of probability distribution
	- used to create new individuals (candidate solutions)

##### What's $m^g$? #card 
- Is the center of the search, the mean
- For the first generation is random $$m^0 \in \mathbb{R}^n$$

##### What's $C^0$? #card 
- Is the first covariance matrix
- Is initialized with identity matrix

##### What's the noise at the beginning of CMA ES? #card 
- Random spherical noise because $$N(m^0, C^0)$$
	- $m^0$ is random
	- $C^0$ is identity matrix

##### What selection method prefer CMA? #card 
- The comma selection
- Because elitism kills momentum
	- we can stuck on local optimum
	- CMA can converge prematurely

##### How CMA creates children? #card 
For all children $k=1..\lambda$:
$$x_k^{g+1} = m^g + \sigma N(0, C^g)$$ where $N(0, C^g)$ is a $C^g$ shaped noise scaled by $\sigma$. 
- So for a given generation $g$ an offspring is distributed as $$x^g_k \sim N(m^g, \sigma^2 C^g)$$

##### How CMA sorts children? #card 
- By fitness value ascending, considering minimization problem
- Best child first

##### How CMA updates mean? #card 
- The new center $m^{g+1}$
- Is the **weighted sum** of the $\mu$ best children $$m^{g+1} = \sum_{i=1}^\mu w_i x_i^{g+1}$$
- Weights are assigned by rank, best children are more influential on the update $$w_1 \ge w_2 \ge \dots \ge w_\mu > 0$$ 
- Mean is normalized$$\sum_{i=1}^\mu w_i= 1$$ the updated mean follows the distribution coherently.

##### How CMA naively updates the matrix? #card 
- $C^{g+1}$ can be estimated by rewriting $m^{g+1}$ as $$m^{g+1}=m^g + \sum_{i=1}^\mu w_i \cdot (x_{i}^{g+1} - m^g)$$ we are adding and subtracting $m^g$
- Reproduce previous updates, but give priority to successful ones $$C_\mu^{g+1} = \sum_i^\mu w_i (x_{i}^{g+1} - m^g)(x_{i}^{g+1} - m^g)^T$$ 
- It's **naive** because we lost previous knowledge, is an estimation just for the current samples  $x_{i}^{g+1}$

##### In the naive CMA updates of matrix, who gives the step direction? #card 
- The $(x_{i}^{g+1} - m^g)(x_{i}^{g+1} - m^g)^T$
- In fact, in naive update, the direction jumps every generation
	- unstable
	- not considering the previous knowledge, just the sample $x_{i}^{g+1}$ one

##### What's rank-$\mu$ update? #card 
- CMA jumpiness updates of  $C^{g+1}$ fix
- Capturing the overall spread of the survival population $\mu$
- is based on **exponential smoothing**
	- we add some new information to the old $C$ $$C^{g+1} = C^g + c_\mu \underbrace{\sum_i^\mu w_i(x_{i}^{g+1} - m^g)(x_{i}^{g+1} - m^g)^T}_{\text{new info}}$$
	- is controlled by learning rate parameter $$c_\mu\in(0,1]$$
		- add just a fraction of the new information
		- or add it all $c_\mu=1$

##### How $c_\mu$ influence CMA matrix update? #card 
- if $c_\mu$ is low
	- $C^{g+1}$ keeps more of the old $C^g$
	- smooth surface
		- slow updates
- if $c_\mu$ is high
	- $C^{g+1}$ changes faster
	- unstable surface

##### What's rank-one update? #card 
- Rank-$\mu$ updates sometimes are too slow
- Rank-one offers a more aggressive update of the matrix
- We update only using the best single step $(x_1^{g+1} - m^g)$ normalized
$$C^{g+1} = (1 - c_\mu) C^g + c_\mu \left(\frac{x_1^{g+1} - m^g}{\sigma}\right)\left(\frac{x_1^{g+1} - m^g}{\sigma}\right)^T$$ following the single best trajectory

##### Why rank-one update is dangerous? #card 
- Can leads to $C$ collapsing
	- the matrix slowly forgets the other trajectories
	- doesn't explore more
		-  after certain mutation may not find the solution anymore

##### Why we combine both rank-$\mu$ and rank-one? #card 
- We use rank-$\mu$ to learn the **global width**
	- expands along best trajectories
		- enough trajectories to span the space
		- not collapsing in one single trajectory line
	- prevents $C$ collapsing (rank-one worst case scenario)
- We use rank-one to elongating $C$
	- quickly learn correlations 
	- stretch the matrix in the direction of the solution.