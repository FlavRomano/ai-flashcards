---
tags:
  - "#card"
  - aif
  - localSearch
---

#flashcards/aif/localSearch

### Systematic vs. Local Search
?
**Systematic Search** (like A*) cares about the path to the goal and explores the state space methodically. **Local Search** only cares about finding the solution (the goal state), does not store the path, and uses very little memory because it doesn't store visited states.
<!--SR:!2026-01-29,3,250-->

### Core Idea of Local Search
?
1. Keep only **one** candidate solution at a time.
2. Iteratively move toward a better neighbor.
3. Stop when the goal (or "good enough") is reached.
<!--SR:!2026-01-29,3,250-->

### The Objective Function
?
In local search, the "landscape" is defined by an objective function. The goal is usually to find the **Global Maximum** (the highest peak). If the function represents cost, we seek the **Global Minimum**.
<!--SR:!2026-01-29,3,250-->

### Local Maxima vs. Global Maxima
?
A **Local Maximum** is a peak that is higher than all its immediate neighbors but lower than the highest point in the landscape (**Global Maximum**). Local search algorithms often get stuck here.
<!--SR:!2026-01-29,3,250-->

### Hill-Climbing (Greedy Local Search)
?
An algorithm that continuously moves in the direction of increasing value (uphill). It terminates when it reaches a "peak" where no neighbor has a higher value.
<!--SR:!2026-01-29,3,250-->

### Limitations of Hill-Climbing
?
It can get stuck on:
1. **Local Optima:** Peaks that aren't the highest.
2. **Ridges:** Narrow slopes where every move feels downhill.
3. **Plateaus (Flat Valleys):** Areas where the landscape is flat, offering no direction for improvement.
<!--SR:!2026-01-29,3,250-->

### Random Restart
?
A strategy to solve the local optima problem by conducting a series of hill-climbing searches, each starting from a randomly chosen initial state. It is "complete" (will find the goal) if given enough time.
<!--SR:!2026-02-02,5,248-->

### Sideways Moves
?
Allowing the search to move to a neighbor with the *same* value as the current state. This helps escape "shoulders" or flat local maxima. Usually capped at a maximum of $K$ steps to prevent infinite loops.
<!--SR:!2026-01-30,2,228-->

### Hill-Climbing and Gradient Descent
?
Hill-climbing is the discrete version of **Gradient Descent**. Gradient Descent uses the gradient of the evaluation function to guide the search and is the foundation of modern deep learning optimization.
<!--SR:!2026-02-02,5,248-->

### Simulated Annealing
?
An algorithm that combines hill-climbing with a **random walk**. It allows "downhill" (bad) moves to escape local optima. The probability of taking a bad move decreases over time as the "temperature" cools.
<!--SR:!2026-02-02,5,248-->

### Temperature ($T$) in Simulated Annealing
?
A parameter that controls the trade-off between exploration and exploitation.
*   **High T:** High probability of accepting bad moves (random walk/exploration).
*   **Low T:** Low probability of accepting bad moves (hill-climbing/exploitation).
<!--SR:!2026-01-29,3,250-->

### Annealing Schedule
?
The strategy for decreasing the temperature ($T$) over time. If $T$ is decreased "slow enough," the algorithm is guaranteed to converge on the global maximum.
<!--SR:!2026-01-29,3,250-->

### Local Beam Search
?
Instead of keeping one state, it tracks **$k$** states. At each step, all successors of all $k$ states are generated, but only the top $k$ among all of them are kept.
<!--SR:!2026-02-02,5,248-->

### Beam Search Interaction
?
Local Beam Search is **not** just $k$ parallel searches. Because the top $k$ states are chosen from the combined pool of all successors, the "useful" information from one successful path can be multiplied while poor paths are abandoned.
<!--SR:!2026-01-29,3,250-->

### Stochastic Beam Search
?
A variation of beam search that picks the next $k$ states **probabilistically** rather than picking the absolute top $k$. A state's chance of being picked is proportional to its value, which helps maintain diversity and prevent clustering at one local maximum.
<!--SR:!2026-01-29,3,250-->

### Encoder-Decoder in NLP
?
In text generation, the **Encoder** processes the input into a context vector. The **Decoder** uses that vector to generate output word-by-word by predicting the probability distribution of the next word.
<!--SR:!2026-01-29,3,250-->

### Beam Search vs. Greedy Search in NLP
?
**Greedy Search** picks the single most likely word at each step (short-sighted). **Beam Search** keeps $N$ best sequences alive at each step, allowing it to find a path that is more probable overall, even if the first word wasn't the top choice.
<!--SR:!2026-01-29,3,250-->

### Nondeterministic Environments
?
Environments where the result of an action is not certain (e.g., the Erratic Vacuum world). An action can result in a set of possible future states rather than a single determined one.
<!--SR:!2026-01-29,3,250-->

### Belief State
?
The set of all possible states the agent thinks it could currently be in. It represents the agent's uncertainty about the environment.
<!--SR:!2026-01-30,2,228-->

### Conditional Plan
?
A solution for nondeterministic environments that is a **tree** (if-then-else logic) rather than a simple sequence. It tells the agent what to do based on which state the environment actually ends up in.
<!--SR:!2026-02-02,5,248-->

### AND-OR Search Trees
?
*   **OR nodes:** Points where the **agent** chooses an action.
*   **AND nodes:** Points where the **environment** provides different possible outcomes for that action.
<!--SR:!2026-01-29,3,250-->

### Solution to an AND-OR Tree
?
A subtree that:
1. Has a goal state at every leaf.
2. Specifies exactly one action for every OR node.
3. Includes every possible outcome branch for every AND node.
<!--SR:!2026-02-02,5,248-->

### Sensorless (Conformant) Problems
?
Problems where the agent has **no percepts** (it is blind). The agent must find a sequence of actions that reaches the goal regardless of which state it started in.
<!--SR:!2026-02-02,5,248-->

### Searching in Belief Space
?
For a state space of size $N$, the **Belief Space** size is $2^N$ (the powerset). The goal is to find a path from the initial belief state (all $N$ states) to a belief state where all members are goal states.
<!--SR:!2026-01-30,2,230-->

### Pruning in Belief Space
?
If you have already found a solution for a belief state $B_1$, you can **prune** (ignore) any branch for a belief state $B_2$ that is a superset of $B_1$. If a plan works for a large set of possibilities, it will also work for any subset of those possibilities.
<!--SR:!2026-01-30,2,230-->

### Incremental Belief-State Search
?
A strategy to solve conformant problems by finding a solution for one state in the belief set and then checking if it works for the others. If it fails, you try again with a different plan. This helps "fail fast" and speed up convergence.
<!--SR:!2026-01-29,3,250-->