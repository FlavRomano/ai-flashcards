---
tags:
  - "#card"
  - aif
  - probability
---

#flashcards/aif/prob3

### Concept Drift Types
?
**Virtual Drift:** The world keeps expanding, but previous knowledge remains valid. You need to *add* new concepts (e.g., unlocking a new area in a game map).
**Real Drift:** The concepts themselves change, making previous knowledge invalid. You need to *update* the agent's knowledge (e.g., a new President is elected).

### Markov Chain Definition
?
A statistical model where a random variable $X$ changes over discrete time steps ($t$). The state at time $t+1$ depends only on the state at time $t$.

### Stationarity Assumption (Time-Homogeneity)
?
The assumption that the rules governing the change in state do not change over time. The transition probabilities are fixed (e.g., the laws of physics don't change from Monday to Tuesday), even though the state itself changes.

### First-Order Markov Assumption
?
The assumption that the future is conditionally independent of the past, given the present.
**Formula:** $P(X_t | X_{0:t-1}) = P(X_t | X_{t-1})$
**Meaning:** To predict the next state, you only need to know the current state. The history of how you got there is irrelevant.

### Hidden Markov Model (HMM) Components
?
An HMM consists of two variables and two models:
1.  **Hidden State ($X_t$):** The unobservable reality (e.g., Rain).
2.  **Evidence Variable ($E_t$):** The observable data (e.g., Umbrella).
3.  **Transition Model:** $P(X_t | X_{t-1})$.
4.  **Sensor (Observation) Model:** $P(E_t | X_t)$.

### Sensor Markov Assumption
?
The assumption that the evidence (observation) at time $t$ depends *only* on the state at time $t$, not on previous states or previous observations.
**Formula:** $P(E_t | X_{0:t}, E_{1:t-1}) = P(E_t | X_t)$

### Inference Task: Filtering
?
**Task:** State Estimation.
Given all evidence up to the current moment ($e_{1:t}$), compute the probability distribution over the current state ($X_t$).
**Goal:** Compute $P(X_t | e_{1:t})$.

### Inference Task: Prediction
?
Given all evidence up to the current moment ($e_{1:t}$), compute the probability distribution over a *future* state ($X_{t+k}$, where $k \geq 1$).

### Inference Task: Smoothing
?
Given all evidence up to the current moment ($e_{1:t}$), compute the probability distribution over a *past* state ($X_k$, where $k < t$).
**Goal:** Use "hindsight" (future evidence) to better estimate what happened in the past.

### Inference Task: Most Likely Explanation
?
Given a sequence of observations ($e_{1:t}$), find the sequence of states ($x_{1:t}$) that was most likely to generate those observations.
**Solved by:** The Viterbi Algorithm.

### Filtering Equation (Recursive)
?
$$P(X_t | e_{1:t}) = \alpha P(e_t | X_t) \sum_{x_{t-1}} P(X_t | x_{t-1}) P(x_{t-1} | e_{1:t-1})$$
It consists of two terms:
1.  **Prediction:** Projects the previous belief forward using the Transition Model.
2.  **Update:** Adjusts the belief using the current Sensor Model and Evidence.

### Computational Complexity of Filtering
?
Filtering scales **linearly** with time ($O(t)$). Because it is recursive, the cost per time step is constant. You do not need to re-process the entire history at every step, only the message from the previous step.

### Matrix Representation in HMMs
?
Transition and Sensor models can be represented as matrices. This allows the use of efficient linear algebra operations (matrix-vector multiplication), which can be parallelized on GPUs/accelerators.

### Prediction Convergence
?
If you predict very far into the future ($k \to \infty$) without new evidence, the probability distribution converges to the **Stationary Distribution** of the Markov process. The initial state is "forgotten."

### Mixing Time
?
The time it takes for a Markov chain to reach its stationary distribution (convergence). If the transition model is very uncertain, the mixing time is short, meaning long-term prediction becomes difficult quickly.

### Likelihood of Evidence
?
The probability of the observation sequence itself occurring: $P(e_{1:t})$.
It is calculated using the forward message ($f_{1:t}$) by summing out the hidden state at the end, without normalizing.

### Underflow Problem
?
When calculating probabilities for long sequences (like Likelihood), multiplying many small numbers (probabilities $< 1$) results in a number so close to zero that the computer rounds it to 0.
**Solution:** Use Log-Probabilities (adding logs instead of multiplying probabilities).

### Forward-Backward Algorithm
?
The algorithm used for **Smoothing**.
It combines:
1.  **Forward message ($f_{1:k}$):** Filtering from start to $k$.
2.  **Backward message ($b_{k+1:t}$):** A recursive message running from the end ($t$) back to $k$.
**Formula:** $P(X_k | e_{1:t}) = \alpha (f_{1:k} \times b_{k+1:t})$

### Initializing the Backward Message
?
When running the backward pass for smoothing, there is no "future evidence" after the final step $t$. Therefore, the backward message $b_{t+1:t}$ is initialized as a **vector of 1s** (representing no information/uniform weight).

### Viterbi Algorithm Purpose
?
To find the single most likely *sequence* of hidden states. It looks for the path with the maximum joint probability. It differs from finding the most likely state for each time step individually (which ignores transition constraints).

### Viterbi vs. Filtering (Math Difference)
?
**Filtering:** Uses the **Sum** over previous states (Law of Total Probability).
**Viterbi:** Uses the **Max** over previous states (Optimization).
Both are recursive dynamic programming algorithms.

### Viterbi Trellis Diagram
?
A graph structure representing states over time. The Viterbi algorithm flows forward through the trellis, keeping pointers to the "best previous state" for each node. The final path is found by **backtracking** (following the pointers backward) from the best final state.