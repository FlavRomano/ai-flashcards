---
tags:
  - "#card"
  - aif
  - bayesian-network
  - probability
---

#flashcards/aif/prob2

### Bayesian Network Definition
?
A Bayesian Network is a **directed, acyclic graph (DAG)** where:
1.  **Nodes** represent random variables.
2.  **Arrows** (directed edges) represent dependency (usually causal influence).
3.  Each node has a **Conditional Probability Table (CPT)** quantifying the effect of its parents ($P(X | Parents(X))$).

### Benefit of Bayesian Networks
?
They allow for the compact representation of the **Full Joint Probability Distribution**. By defining the structure and local interactions, you avoid the combinatorial explosion required to store a full joint probability table.

### CPT Size (Boolean Variables)
?
If a Boolean node $X$ has $k$ Boolean parents, how many rows does its Conditional Probability Table (CPT) contain?
**$2^k$ rows.**
(Note: We only need to store the probability of $X=True$; the probability of $X=False$ is implicitly $1 - P(True)$).

### Global Semantics (Joint Distribution)
?
How is the full joint distribution $P(x_1, \dots, x_n)$ defined in a Bayesian Network?
It is the **product** of the local conditional distributions:
$$ P(x_1, \dots, x_n) = \prod_{i} P(x_i | Parents(x_i)) $$

### Local Semantics
?
A node $X$ is conditionally independent of its **\_\_\_\_\_\_\_\_\_\_** given its **parents**.
**Nondescendants**.

### Markov Blanket
?
What constitutes the **Markov Blanket** of a node $X$?
1.  Parents of $X$
2.  Children of $X$
3.  Parents of the Children of $X$ (Spouses)

**Property:** $X$ is conditionally independent of *all other nodes* in the network given its Markov Blanket.

### Constructing a BN (Ordering)
?
When building a Bayesian Network using the Chain Rule, which variable ordering results in the most compact network?
**Causal Ordering:** Order variables from **Causes** to **Effects**.
(Incorrect ordering can lead to unnecessary links and larger CPTs).

### Canonical Distributions
?
What are **Canonical Distributions** used for?
They are standard patterns used to compactly define CPTs when there are too many parents or continuous variables, avoiding the need to specify every single table entry manually (e.g., Deterministic nodes, Noisy-OR).

### Deterministic Nodes
?
What is a Deterministic Node?
A node where the value is exactly determined by the values of its parents, with no uncertainty.
Example: $X = f(Parents(X))$.

### Context-Specific Independence (CSI)
?
What is Context-Specific Independence?
When a variable is conditionally independent of some parents given **certain values** of other parents.
Example: $P(Damage | Accident, Use)$. If $Accident=True$, the variable $Use$ might become irrelevant.

### Noisy-OR Assumptions
?
What are the two key assumptions of the **Noisy-OR** model?
1.  **All causes are listed** (or covered by a leak node).
2.  **Inhibition Independence:** The mechanism by which one cause acts is independent of the others. A cause fails to trigger the effect with a specific probability $q_i$.

### Noisy-OR Formula
?
In a Noisy-OR model, what is the formula for the probability that the Effect is **True**, given a set of active parents?
$$ P(Effect | Parents) = 1 - \prod_{j} q_j $$
Where $q_j$ is the **inhibition probability** of the active parent $j$ (the probability that parent $j$ is True but fails to trigger the effect).

### Leak Node
?
What is a **Leak Node** in the context of Noisy-OR?
A background node that is always True. It is added to cover "all other unknown causes" not explicitly modeled in the network.

### Hybrid Bayesian Networks
?
What is a Hybrid Bayesian Network?
A network that contains both **Discrete** and **Continuous** variables.

### Linear Gaussian Model
?
How is a Continuous Node with Continuous Parents often modeled?
Using a **Linear Gaussian Model**: The child has a Gaussian distribution where the **mean ($\mu$)** varies linearly with the value of the parent.
$\mu = a \cdot parent + b$

### Soft Threshold Function
?
How do we model a **Discrete** child node with a **Continuous** parent?
Using a **Soft Threshold Function** (e.g., Logistic/Sigmoid function).
$$ P(Child=True | Parent) = \frac{1}{1 + e^{-Parent}} $$
This creates a smooth transition ("S" curve) from False to True as the continuous parent increases.

### Inference by Enumeration (Concept)
?
How does Inference by Enumeration calculate the posterior probability $P(Query | Evidence)$?
It sums the joint probability over all possible combinations of values for the **Hidden Variables**.
$$ P(Q|E) = \alpha \sum_{hidden} P(Q, E, hidden) $$

### Inference by Enumeration (Complexity)
?
What is the time complexity of Inference by Enumeration in the worst case?
**$O(d^n)$** (Exponential).
It is inefficient because it repeats the calculation of the same sub-expressions multiple times (re-evaluating the same branches of the tree).

### Variable Elimination
?
What is **Variable Elimination**?
A Dynamic Programming algorithm for exact inference. It avoids repeated computations by evaluating expressions from right-to-left and storing intermediate results as **Factors**.

### Operations in Variable Elimination
?
What are the two main operations performed on Factors during Variable Elimination?
1.  **Pointwise Product:** Multiplying matrices/factors to combine them.
2.  **Summing Out:** Summing probabilities over a variable to remove it from the factor.

### Polytree (Singly-Connected Network)
?
What is a **Polytree** and why is it important for inference?
A network where there is at most one undirected path between any two nodes (no loops).
**Importance:** Exact inference in Polytrees is **Linear time $O(n)$**.

### NP-Hardness of Inference
?
What is the worst-case computational complexity of Exact Inference in a general (multiply connected) Bayesian Network?
**NP-Hard**.

### Direct Sampling
?
How does Direct Sampling work?
It simulates the network by generating random events for each node in topological order (parents before children), based on their CPT probabilities.

### Rejection Sampling
?
How does **Rejection Sampling** estimate $P(Query | Evidence)$?
1.  Generate $N$ samples from the network.
2.  **Reject** (discard) all samples where the generated values do not match the **Evidence**.
3.  Count the frequency of the **Query** in the remaining samples.

### Limitation of Rejection Sampling
?
When does Rejection Sampling perform poorly?
When the **Evidence is rare** (low probability).
Most samples will be rejected, requiring a massive number of iterations to get a stable estimate.

### Causal Network vs. Standard BN
?
What is the difference between a Standard BN and a Causal Network regarding arrow direction?
In a Standard BN, arrows represent correlation/dependency.
In a Causal Network, arrows **must** represent the direction of **causality** (Cause $\rightarrow$ Effect).

### The `do` Operator
?
What does the operator $do(X=x)$ represent?
It represents an **Intervention** (acting on the world to force variable $X$ to value $x$), as opposed to an **Observation** (seeing that $X$ happens to be $x$).

### Graph Surgery (Intervention)
?
How does computing $P(Y | do(X=x))$ differ structurally from computing $P(Y | X=x)$?
**Observation ($X=x$):** Information flows through arrows to parents (diagnostics).
**Intervention ($do(X=x)$):** We **remove** the arrows pointing to $X$ from its parents (Intervention breaks the dependence on causes). $X$ is fixed to $x$.

### Back-door Criterion
?
What is the purpose of the **Back-door Criterion** in Causal Networks?
It helps identify which variables must be controlled for (confounders) to compute the true causal effect of an intervention, without needing a randomized controlled trial.