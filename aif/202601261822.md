---
tags:
  - "#card"
  - aif
  - games
---

#flashcards/aif/games

### Definition of Adversarial Search
?
**Adversarial search** refers to a search strategy in a competitive environment where multiple agents have conflicting goals. It is commonly used in two-player zero-sum games where one player's gain is the other player's loss (e.g., if Player A wins +1, Player B loses -1).

### Game Formalization Components
?
A game is defined by the following elements:
*   **$S_0$:** The initial state.
*   **TO-MOVE(s):** The player whose turn it is in state $s$.
*   **ACTIONS(s):** The set of legal moves in state $s$.
*   **RESULT(s, a):** The transition model returning the new state after action $a$.
*   **IS-TERMINAL(s):** Checks if the game is over.
*   **UTILITY(s, p):** The payoff/score for player $p$ at terminal state $s$.

### Zero-Sum Game
?
A game where the total utility is constant. There is no "win-win" situation. The players have diametrically opposed goals: maximizing their own utility implies minimizing the opponent's utility.

### Perfect Information vs. Imperfect Information
?
*   **Perfect Information:** The environment is fully observable (e.g., Chess, Go, Othello). All players can see the entire state of the game.
*   **Imperfect Information:** Parts of the game state are hidden from players (e.g., Poker, Scrabble, Battleship).

### Minimax: MAX vs. MIN Players
?
In Minimax, there are two players:
*   **MAX:** The agent trying to maximize the utility value.
*   **MIN:** The opponent, who tries to minimize the utility value (or maximize their own win, which is a loss for MAX).
MAX assumes MIN plays optimally (perfect play).

### The Minimax Value (Algorithm Logic)
?
The Minimax value of a node is determined recursively:
1.  **Terminal Node:** Returns the Utility value.
2.  **MAX Node:** Returns the maximum value of its children (chooses the best move for itself).
3.  **MIN Node:** Returns the minimum value of its children (chooses the worst move for MAX).

### Minimax Time Complexity
?
**$O(b^m)$**
Where $b$ is the branching factor (legal moves) and $m$ is the maximum depth of the tree.
This makes Minimax impractical for games with high branching factors or depth (like Chess or Go) without optimization.

### Minimax Space Complexity
?
**$O(bm)$**
Minimax performs a Depth-First Search (DFS), so it only needs to store the current path and siblings on the path, resulting in linear space complexity.

### Minimax Completeness and Optimality
?
*   **Complete:** Yes, if the game tree is finite.
*   **Optimal:** Yes, assuming the opponent plays perfectly.

### Playing Against Suboptimal Opponents
?
Standard Minimax assumes the opponent plays perfectly.
If an opponent is suboptimal (makes mistakes), Minimax will still guarantee a non-losing result (if possible), but it might not choose a "risky" move that could exploit the opponent's mistake to win faster. It always prepares for the worst-case scenario.

### Minimax for >2 Players
?
In games with 3 or more players, the single utility value is replaced by a **vector of values**.
Each node returns a vector $<v_A, v_B, v_C>$ representing the utility for Player A, Player B, and Player C respectively.

### Alpha-Beta Pruning: Goal
?
The goal of Alpha-Beta pruning is to reduce the **Time Complexity** of the search by "pruning" (ignoring) branches of the game tree that strictly cannot influence the final decision. It yields the exact same result as standard Minimax.

### Definition of Alpha ($\alpha$)
?
**$\alpha$** is the value of the best (highest-value) choice found so far at any choice point along the path for **MAX**.
(MAX will strictly avoid any move worth less than $\alpha$).

### Definition of Beta ($\beta$)
?
**$\beta$** is the value of the best (lowest-value) choice found so far at any choice point along the path for **MIN**.
(MIN will strictly avoid any move worth more than $\beta$).

### Alpha-Beta Pruning Rule
?
Pruning occurs when **$\alpha \geq \beta$**.
*   If a MIN node finds a value less than or equal to $\alpha$, it stops looking (MAX won't let it reach this state).
*   If a MAX node finds a value greater than or equal to $\beta$, it stops looking (MIN won't let it reach this state).

### Alpha-Beta Complexity with Best Ordering
?
If moves are ordered perfectly (examining the best moves first), the time complexity drops to **$O(b^{m/2})$**.
This effectively doubles the searchable depth compared to standard Minimax within the same time limit.

### Heuristic Evaluation Functions (Type A Strategy)
?
Used when the search space is too deep to reach terminal nodes.
Instead of searching to the end, the algorithm cuts off at a **fixed depth** and uses a heuristic function to estimate the utility of that state (e.g., material value in Chess).

### The Horizon Effect
?
A problem in depth-limited search where a heuristic evaluates a state as "good" because a negative consequence (like losing a piece) is pushed just beyond the search horizon (depth limit). The program effectively "delays" the inevitable loss without seeing it.

### Monte-Carlo Tree Search (MCTS): Use Case
?
MCTS is best used when:
1.  The branching factor is extremely large (e.g., Go), making Minimax/Alpha-Beta ineffective.
2.  There is no reliable heuristic evaluation function available.

### Rollout / Playout
?
A simulation step in MCTS where the algorithm plays out the game from a specific state to the very end using a default policy (often random or semi-random). The final result (win/loss) is then used to value the state.

### MCTS: 4 Steps
?
1.  **Selection:** Traverse the tree from the root to a leaf using a selection policy (like UCT).
2.  **Expansion:** Add one or more child nodes to the current leaf.
3.  **Simulation:** Perform a rollout from the new node to determine a result.
4.  **Back-propagation:** Update the win/loss statistics for all nodes along the path back to the root.

### Exploitation vs. Exploration in MCTS
?
*   **Exploitation:** Choosing moves that have high average win rates (sticking to what works).
*   **Exploration:** Choosing moves that have few playouts to see if they are better than they look (reducing uncertainty).
The algorithm must balance these two.

### UCT (Upper Confidence Bounds applied to Trees)
?
A selection policy formula used in MCTS to balance exploration and exploitation.
$UCB1(n) = \frac{U(n)}{N(n)} + C \sqrt{\frac{\log N(Parent(n))}{N(n)}}$

### UCT: Exploitation Term
?
**$\frac{U(n)}{N(n)}$**
This represents the average utility (win rate) of the node. High values encourage picking known winning moves.

### UCT: Exploration Term
?
**$C \sqrt{\frac{\log N(Parent(n))}{N(n)}}$**
This term is high when a node has been visited rarely ($N(n)$ is low) compared to its parent. It forces the algorithm to visit unexplored branches. As $N(n)$ increases, this term goes to zero.

### MCTS Convergence
?
Given infinite time and memory, MCTS converges to the optimal solution (Minimax equivalent).
In practice, the move with the highest number of **playouts** (simulations) is usually selected as the best move.

### Stochastic Games
?
Games that involve an element of chance (e.g., dice in Backgammon, shuffling in Bridge).
These games require **Chance Nodes** in the game tree, which represent possible outcomes of a random event.

### Expected Minimax
?
The algorithm used for stochastic games.
*   MAX and MIN nodes work as usual.
*   **Chance Nodes** calculate the **Expected Value**: the sum of the values of all children weighted by their probability.
$ExpectedValue(s) = \sum P(r) \times Value(Result(s, r))$